workflow: session-insights-dev
description: Development workflow for session-insights CLI tool with failure diagnosis

agents:
  - role: engineer
  - role: reviewer

states:
  start:
    type: initial
    auto_transition: engineering

  engineering:
    instructions: |
      Implement the assigned task for the session-insights CLI tool.

      TASK ALIGNMENT CHECK (do this FIRST, before writing any code):
      Identify which KPI(s) this task will move. If you cannot name at least one
      of the KPIs below that this task directly advances, STOP and signal blocked
      with an explanation. Do not execute tasks that only improve infrastructure
      without moving a metric.

      KPIs (updated after cycle 4 — 68% overall):
      - note_content_richness: 0% — CRITICAL, #1 priority
      - vermas_task_visibility: 65% — needs edge-case handling
      - cli_runs_clean: 85% — needs negative-path tests
      - obsidian_compatibility: on track

      MANDATORY PRIORITY: note_content_richness
      This KPI has had ZERO movement across all cycles. The PARSER layer is
      COMPLETE AND STABLE (confirmed cycle 3-6). Do NOT modify parsers.
      The gap is in the FORMATTER/RENDERER layer: parsed data exists but is not
      being translated into rich note output. Specifically wire these fields:
      - Timestamps (formatted correctly)
      - Durations (calculated from start/end)
      - Agent interactions (with sufficient detail)
      - Task metadata (status, outcomes)

      For vermas_task_visibility tasks:
      Older VerMAS session formats in .vermas/state/ may have structural
      differences (metadata fields, naming conventions) not yet handled.
      Investigate legacy format structure before assuming parser bugs.

      For cli_runs_clean tasks:
      Happy path is solid. Focus on NEGATIVE-PATH testing:
      - Malformed inputs
      - Missing directories
      - Permission errors
      - Unexpected file formats

      Follow Python best practices:
      - Use Pydantic v2 for models
      - Click for CLI commands
      - pytest for testing
      - Type hints throughout

      Run tests before signaling done:
      `uv run pytest tests/ -x -q`

      If tests fail, fix them before signaling.

      Signal "done" when implementation is complete and tests pass.
      Signal "blocked" if you encounter infrastructure issues or cannot
      identify which KPI this task advances.
    on_signal:
      done:
        from: engineer
        next: reviewing
      blocked:
        from: engineer
        next: blocked
    timeout:
      duration: 30m
      action: nudge

  reviewing:
    instructions: |
      Review the implementation for correctness and quality.

      Verification steps:
      1. Read all changed files
      2. Run full test suite with coverage
      3. Test CLI manually if applicable
      4. Check for edge cases and error handling

      CRITICAL REVIEW CHECKS:
      - If the task targets note_content_richness: verify that the formatter
        actually renders the claimed fields (timestamps, durations, agent
        interactions, task metadata) into the output. Do not approve if
        parsed data is available but not surfaced in rendered notes.
      - If the task targets cli_runs_clean: verify negative-path tests exist
        (malformed input, missing dirs, permission errors).
      - Verify no parser modifications were made unless explicitly required
        by the task — the parser layer is stable and should not be touched.

      Signal "approved" if quality is acceptable.
      Signal "needs_revision" with specific feedback if changes needed.
    on_signal:
      approved:
        from: reviewer
        next: finalize
      needs_revision:
        from: reviewer
        next: engineering
    timeout:
      duration: 20m
      action: nudge

  finalize:
    type: terminal
    on_entry:
      - action: commit

  blocked:
    type: terminal
    on_entry:
      - action: log
        message: Workflow blocked - task does not advance any KPI or infrastructure issue encountered
