workflow: session-insights-dev
description: Development workflow for session-insights CLI tool with KPI-aware execution

agents:
  - role: engineer
  - role: reviewer

states:
  start:
    type: initial
    auto_transition: engineering

  engineering:
    instructions: |
      Implement the assigned task for the session-insights CLI tool.

      STEP 0 — FAILURE GATE (before writing any code):
      Check whether this exact task has failed in a previous cycle.
      If it has:
      1. Read the failure logs from the previous attempt.
      2. Identify the SPECIFIC root cause (not just "test failed").
      3. Write a 2-3 sentence diagnosis explaining what went wrong.
      4. Only proceed if you have a DIFFERENT approach than what was tried before.
      5. If you cannot identify a different approach, signal "blocked" with your
         diagnosis so the mission can escalate rather than waste another cycle.

      STEP 1 — KPI AWARENESS:
      Before implementing, identify which KPI(s) this task advances:
      - narrative_quality: Rich narrative output from session analysis
      - project_notes: Useful project-level notes and summaries
      - weekly_digests: Weekly digest generation (100% target, currently ~0%)
      - project_detection: Accurate project detection from session data
      - tests_pass: Test suite passing with coverage

      If this task does NOT clearly advance at least one KPI, raise a concern
      in your done signal so the reviewer can evaluate whether to proceed.

      STEP 2 — MEASUREMENT FIRST:
      If the task targets an unmeasured KPI (e.g., project_detection), implement
      or verify the measurement infrastructure FIRST before building features.
      You cannot improve what you cannot measure.

      STEP 3 — IMPLEMENTATION:
      Follow Python best practices:
      - Use Pydantic v2 for models
      - Click for CLI commands
      - pytest for testing
      - Type hints throughout

      Prioritize end-user-facing functionality over internal robustness.
      Parser work is complete and stable — focus on downstream features
      that consume parser output (digests, notes, narratives, detection).

      STEP 4 — VERIFICATION:
      Run tests before signaling done:
      `uv run pytest tests/ -x -q`

      If tests fail, fix them before signaling.

      Signal "done" when implementation is complete and tests pass.
      Signal "blocked" if you encounter infrastructure issues or cannot
      find a new approach for a previously-failed task.
    on_signal:
      done:
        from: engineer
        next: reviewing
      blocked:
        from: engineer
        next: blocked
    timeout:
      duration: 30m
      action: nudge

  reviewing:
    instructions: |
      Review the implementation for correctness, quality, AND KPI impact.

      Verification steps:
      1. Read all changed files
      2. Run full test suite with coverage: `uv run pytest tests/ -x -q --cov`
      3. Test CLI manually if applicable
      4. Check for edge cases and error handling
      5. CRITICAL — KPI impact check: Does this change meaningfully advance
         at least one KPI? If the task completed but moved no KPIs forward,
         flag this in your review signal.

      Reject work that is purely internal refactoring or test-only when
      KPI-advancing work is available and was assigned.

      Signal "approved" if quality is acceptable and KPI impact is clear.
      Signal "needs_revision" with specific feedback if changes needed.
    on_signal:
      approved:
        from: reviewer
        next: finalize
      needs_revision:
        from: reviewer
        next: engineering
    timeout:
      duration: 15m
      action: nudge

  blocked:
    type: terminal
    on_entry:
      - action: log
        message: Workflow blocked - failure gate triggered or infrastructure issue

  finalize:
    type: terminal
    on_entry:
      - action: commit
